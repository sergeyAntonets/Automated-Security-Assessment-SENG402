{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b6af60-f174-4dc7-a6b8-2ad51d8af784",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, Replicate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76192d8",
   "metadata": {},
   "source": [
    "## Install All Required Packages\n",
    "Run this cell first to install all necessary packages if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install openai replicate python-dotenv pandas numpy matplotlib google.genai transformers torch bitsandbytes llamaapi accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9372cb3",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, Replicate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b88102b-a07f-4763-9fcb-a2bd248d00e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4d4dd8e19f4041b0e378aa82a10357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import replicate\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "from runningLLAMA import llama_local_generate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd791ee-3c67-4eb6-8678-827dfbf3bbb7",
   "metadata": {},
   "source": [
    "## Setup all APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3d6a19-2b3a-4918-8c69-98313903ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10403f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama api\n",
    "llama_client = OpenAI(\n",
    "api_key = os.getenv(\"LLAMA_API_KEY\"),\n",
    "base_url = \"https://api.llmapi.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae925af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grok\n",
    "grok_client = OpenAI(\n",
    "  api_key=os.getenv(\"GROK_API_KEY\"),\n",
    "  base_url=\"https://api.x.ai/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9787e703-fd56-4d6f-82d1-e00ef4137503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "# openai_client = OpenAI(api_key=os.environ.get(\"OPEN_AI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0eb4ea-6fea-407a-8de1-7f2ed22b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "# project_id = \"\"   # add project ID and location\n",
    "# vertexai.init(project=project_id, location=\"\")\n",
    "\n",
    "# gemini_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f299-880a-45d6-88a4-7b33448a7fb3",
   "metadata": {},
   "source": [
    "## Prediction Params & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5141f2a9-a4d2-4d57-b043-9873d213dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for more deterministic output\n",
    "temperature = 0\n",
    "top_p = 1\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6e074a-85de-48c7-982e-339dbce560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb01b07-3377-4845-a5a9-74719a86ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    'llama3-70b': 'meta/meta-llama-3-70b-instruct',\n",
    "    'llama3-8b': 'meta/meta-llama-3-8b-instruct',\n",
    "    'api-llama3.1-8b': 'llama3.1-8b',\n",
    "    'gemini-2.0-flash': 'gemini-2.0-flash', \n",
    "    'gemini-2.5-pro': 'gemini-2.5-pro-exp-03-25', \n",
    "    'gpt3': 'gpt-3.5-turbo',\n",
    "    'gpt4': 'gpt-4-turbo',\n",
    "    'gpt-4o-mini': 'gpt-4o-mini',\n",
    "    'grok-3-beta': 'grok-3-beta',\n",
    "}\n",
    "# look at llama3.3-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4890ce6-7c73-4512-9316-64d49cc20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(question, model_name):\n",
    "    if model_name.startswith('mistral'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': temperature, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('gemma'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': 0.01, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('01-ai'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': temperature, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('gpt'):\n",
    "        # ChatGPT\n",
    "        model = model_mapping[model_name]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model_name.startswith('gemini'):\n",
    "        # Gemini\n",
    "        model = model_mapping[model_name]\n",
    "\n",
    "        # Create message content\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=sys_prompt + \" \" + question)])\n",
    "        ]        \n",
    "        \n",
    "        safety_settings = [\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "        ]\n",
    "       \n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_output_tokens=max_tokens,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "       \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        )\n",
    "\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = gemini_client.models.generate_content(\n",
    "                    model=model,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "                output = response.candidates[0].content.parts[0].text\n",
    "                time.sleep(1)  # Regular delay between requests\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"429 RESOURCE_EXHAUSTED\" in error_str:\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit hit, retrying in 2 seconds... (attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds as suggested by API\n",
    "                    if retry_count == max_retries:\n",
    "                        output = f\"Error: Rate limit exceeded after {max_retries} attempts.\"\n",
    "                else:\n",
    "                    output = f\"Error: {str(e)}\"\n",
    "                    break\n",
    "                \n",
    "    elif model_name.startswith('llama-local'):\n",
    "        output = llama_local_generate(sys_prompt, question, max_tokens=max_tokens, temperature=0.01, top_p=top_p, seed=seed)\n",
    "\n",
    "    elif model_name.startswith('api-llama3.1-8b'):\n",
    "         model = model_mapping[model_name]\n",
    "         response = llama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=0.01,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "         output = response.choices[0].message.content\n",
    "\n",
    "    elif model_name.startswith('grok-3-beta'):\n",
    "        model = model_mapping[model_name]\n",
    "        response = grok_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        \n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb481fe-ff69-42af-8306-4eb4f5d48973",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26cd8376-21b9-4c91-b86d-876f1da04260",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"Analyze the following CVE description and map it to the appropriate CWE. \"\n",
    "    \"Provide a brief justification. The last line of your answer should only contain the CWE ID.\\n\\n\"\n",
    "    \"CVE Description:\\n\\n\"\n",
    "    \"Dell EMC CloudLink 7.1 and all prior versions contain an Improper Input Validation Vulnerability. \"\n",
    "    \"A remote low privileged attacker may potentially exploit this vulnerability, \"\n",
    "    \"leading to execution of arbitrary files on the server.\"\n",
    ")\n",
    "\n",
    "question2 = (\n",
    "    \"Analyze the following CVE description and calculate the CVSS v3.1 Base Score. \"\n",
    "    \"Determine the values for each base metric: AV, AC, PR, UI, S, C, I, and A. \"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string.   \"\n",
    "    \"Valid options for each metric are as follows: \\n\"\n",
    "    \"- **Attack Vector (AV)**: Network (N), Adjacent (A), Local (L), Physical (P)\\n\"\n",
    "    \"- **Attack Complexity (AC)**: Low (L), High (H)\\n\"\n",
    "    \"- **Privileges Required (PR)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **User Interaction (UI)**: None (N), Required (R)\\n\"\n",
    "    \"- **Scope (S)**: Unchanged (U), Changed (C)\\n\"\n",
    "    \"- **Confidentiality (C)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Integrity (I)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Availability (A)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string. \"\n",
    "    \"Ensure the final line of your response contains ONLY the CVSS v3 Vector String (no other text) \"\n",
    "    \"in the following format:  \\n\"\n",
    "    \"Example format: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\\n\\n\"\n",
    "    \"CVE Description: In the Linux kernel through 6.7.1, there is a use-after-free \"\n",
    "    \"in cec_queue_msg_fh, related to drivers/media/cec/core/cec-adap.c and \"\n",
    "    \"drivers/media/cec/core/cec-api.c.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b473c-9b93-46f9-9ac5-6fb8e3fe314c",
   "metadata": {},
   "source": [
    "##### Are all the APIS working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5ac66-c0a8-4a32-81b0-4c45d68092ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CVSS v3.1 Base Score is calculated as follows:\n",
      "- Attack Vector (AV): Network (N)\n",
      "- Attack Complexity (AC): Low (L)\n",
      "- Privileges Required (PR): None (N)\n",
      "- User Interaction (UI): None (N)\n",
      "- Scope (S): Unchanged (U)\n",
      "- Confidentiality (C): High (H)\n",
      "- Integrity (I): High (H)\n",
      "- Availability (A): High (H)\n",
      "The final CVSS v3.1 vector string is: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    }
   ],
   "source": [
    "# print(get_single_prediction(question2, 'llama-local'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6699f587-4e5c-4a59-a545-265fa4762fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_single_prediction(question2, 'grok-3-beta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57c22fb5-bb3d-47f1-bbf2-da27188a0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_single_prediction(question, 'llama3-8b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c673-8d44-43b7-a04a-73f1ec36e2a9",
   "metadata": {},
   "source": [
    "# Run Evaluation for a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580bd3-20e4-4952-af00-e49810c7703c",
   "metadata": {},
   "source": [
    "### All formatting comes here\n",
    "While these captures most output format of the LLMs we studied, we still had to manually collect some responses from the generated response file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "176c5b73-af68-443f-8b80-f1cfec5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rcm(text):\n",
    "    # Define the regex pattern for CWE ID\n",
    "    cwe_pattern = r'CWE-\\d+'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cwe_pattern, text)\n",
    "\n",
    "    # Return the last match if any match is found, otherwise return the original text\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        return text, False\n",
    "\n",
    "def format_vsp(text):\n",
    "    # Define the regex pattern for CVSS v3.1 vector string\n",
    "    #cvss_pattern = r'AV:[^/]+?/AC:[^/]+?/PR:[^/]+?/UI:[^/]+?/S:[^/]+?/C:[^/]+?/I:[^/]+?/A:[^/]+?'\n",
    "    cvss_pattern = r'AV:[A-Za-z]+/AC:[A-Za-z]+/PR:[A-Za-z]+/UI:[A-Za-z]+/S:[A-Za-z]+/C:[A-Za-z]+/I:[A-Za-z]+/A:[A-Za-z]+'\n",
    "\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cvss_pattern, text)\n",
    "\n",
    "    # Return the last match if any match is found, otherwise return the original text\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        return text, False\n",
    "\n",
    "def format_mcq(text):\n",
    "    last_line = text.split('\\n')[-1].rstrip()\n",
    "    if last_line.startswith('A)') or last_line.startswith('B)') or last_line.startswith('C)') or last_line.startswith('D)'):\n",
    "        return last_line[0]\n",
    "    if last_line.endswith('A') or last_line.endswith('B') or last_line.endswith('C') or last_line.endswith('D'):\n",
    "        return last_line[-1]\n",
    "    if last_line.endswith('**'):\n",
    "        return last_line[-3]\n",
    "    if len(last_line) == 0:\n",
    "        last_line = text.split('\\n')[-2].rstrip()\n",
    "        if last_line.startswith('A)') or last_line.startswith('B)') or last_line.startswith('C)') or last_line.startswith('D)'):\n",
    "            return last_line[0]\n",
    "        if last_line.endswith('A') or last_line.endswith('B') or last_line.endswith('C') or last_line.endswith('D'):\n",
    "            return last_line[-1]\n",
    "        if last_line.endswith('**'):\n",
    "            return last_line[-3]\n",
    "    return ' '.join(text.split('\\n'))\n",
    "\n",
    "def format_taa(text):\n",
    "    # need to manually extract the attribution\n",
    "    return ' '.join(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6068fb-49a8-4d80-b7d4-0d5f58750a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path, task, model_name):\n",
    "    # Keep track of time and total #chars generated\n",
    "    start_time = time.time()\n",
    "    count_chars = 0\n",
    "    instructions_failed = 0\n",
    "    \n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    # response contain the entire response, result the formatted result\n",
    "    all_responses = []\n",
    "    all_results = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        try:\n",
    "            output = get_single_prediction(prompt, model_name)\n",
    "            \n",
    "            count_chars += len(output)\n",
    "            all_responses.append(output)\n",
    "            if task == 'rcm':\n",
    "                answer, success = format_rcm(output)\n",
    "                if not success:\n",
    "                    instructions_failed += 1\n",
    "            elif task == 'vsp':\n",
    "                answer, success = format_vsp(output)\n",
    "                if not success:\n",
    "                    instructions_failed += 1      \n",
    "            elif task == 'mcq':\n",
    "                answer = format_mcq(output)\n",
    "            elif task == 'taa':\n",
    "                answer = format_taa(output)\n",
    "            else:\n",
    "                raise ValueError('Task unknown!')\n",
    "        except Exception as e:\n",
    "            answer = 'Error'\n",
    "            all_responses.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "        all_results.append(answer)\n",
    "        print(index+1, answer)\n",
    "        # print(index+1)\n",
    "\n",
    "\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Create responses-new directory if it doesn't exist\n",
    "    output_dir = 'responses-new'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save all the responses & results to the responses-new folder\n",
    "    out_response = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_' + model_name + '_response.txt')\n",
    "    out_result = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_' + model_name + '_result.txt')\n",
    "\n",
    "    with open(out_response, 'w', encoding='utf-8') as f:\n",
    "        out_str = ''\n",
    "        for i in range(len(all_responses)):\n",
    "            out_str += '#####' + str(i+1) + '#####\\n'\n",
    "            out_str += all_responses[i]\n",
    "            out_str += '\\n\\n'\n",
    "        f.write(out_str)\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef874d9-4a0a-4889-971b-3781410b0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6665c28d-4d41-4b98-b150-94d12e9c3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-mcq.tsv', 'mcq', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d94b102e-4fef-4d9e-8dc3-250b9b7edcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-rcm.tsv', 'rcm', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3123f72-0838-4fc1-8ebe-912219909344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n",
      "4 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n",
      "8 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n",
      "Time taken: 87.92103290557861\n",
      "#Characters generated: 28542\n",
      "#Instructions failed: 0\n",
      "------- Done --------\n"
     ]
    }
   ],
   "source": [
    "run_evaluation('../new-data/cti-vsp-only-2024-and-2025-BIG.tsv', 'vsp', 'llama-local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "752819a6-5082-4e80-9501-c65720079fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-taa.tsv', 'taa', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9be07b-6030-46c5-8442-d743875e8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
