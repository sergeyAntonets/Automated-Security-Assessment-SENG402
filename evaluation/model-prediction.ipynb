{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b6af60-f174-4dc7-a6b8-2ad51d8af784",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, LLAMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76192d8",
   "metadata": {},
   "source": [
    "### Install All Required Packages\n",
    "Run this cell first to install all necessary packages if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install openai replicate python-dotenv pandas numpy matplotlib google.genai transformers torch bitsandbytes llamaapi accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9372cb3",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, Replicate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88102b-a07f-4763-9fcb-a2bd248d00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "from runningLLAMA import llama_local_generate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd791ee-3c67-4eb6-8678-827dfbf3bbb7",
   "metadata": {},
   "source": [
    "## Setup all APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d6a19-2b3a-4918-8c69-98313903ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10403f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama api\n",
    "llama_client = OpenAI(\n",
    "api_key = os.getenv(\"LLAMA_API_KEY\"),\n",
    "base_url = \"https://api.llmapi.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787e703-fd56-4d6f-82d1-e00ef4137503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPEN_AI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0eb4ea-6fea-407a-8de1-7f2ed22b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "gemini_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f299-880a-45d6-88a4-7b33448a7fb3",
   "metadata": {},
   "source": [
    "## Prediction Params & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f2a9-a4d2-4d57-b043-9873d213dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for more deterministic output\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e074a-85de-48c7-982e-339dbce560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb01b07-3377-4845-a5a9-74719a86ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    'api-llama3.1-8b': 'llama3.1-8b',\n",
    "    'api-llama3.3-70b': 'llama3.3-70b',\n",
    "    'gemini-2.0-flash': 'gemini-2.0-flash', \n",
    "    'gpt-4o-mini': 'gpt-4o-mini',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4890ce6-7c73-4512-9316-64d49cc20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(question, model_name):\n",
    "    if model_name.startswith('gpt'):\n",
    "        # ChatGPT\n",
    "        model = model_mapping[model_name]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model_name.startswith('gemini'):\n",
    "        # Gemini\n",
    "        model = model_mapping[model_name]\n",
    "\n",
    "        # Create message content\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=sys_prompt + \" \" + question)])\n",
    "        ]        \n",
    "        \n",
    "        safety_settings = [\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "        ]\n",
    "       \n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_output_tokens=max_tokens,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "       \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        )\n",
    "\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = gemini_client.models.generate_content(\n",
    "                    model=model,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "                output = response.candidates[0].content.parts[0].text\n",
    "                time.sleep(1)  # Regular delay between requests\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"429 RESOURCE_EXHAUSTED\" in error_str:\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit hit, retrying in 2 seconds... (attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds as suggested by API\n",
    "                    if retry_count == max_retries:\n",
    "                        output = f\"Error: Rate limit exceeded after {max_retries} attempts.\"\n",
    "                else:\n",
    "                    output = f\"Error: {str(e)}\"\n",
    "                    break\n",
    "                \n",
    "    elif model_name.startswith('llama-local'):\n",
    "        # temperature must be strictly positive (< 0) or will get error\n",
    "        output = llama_local_generate(sys_prompt, question, max_tokens=max_tokens, temperature=0.01, top_p=top_p, seed=seed)\n",
    "\n",
    "    elif model_name.startswith('api-llama'):\n",
    "         model = model_mapping[model_name]\n",
    "         response = llama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=0.01,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "         output = response.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported or not found in model_mapping\")\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb481fe-ff69-42af-8306-4eb4f5d48973",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd8376-21b9-4c91-b86d-876f1da04260",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"Analyze the following CVE description and calculate the CVSS v3.1 Base Score. \"\n",
    "    \"Determine the values for each base metric: AV, AC, PR, UI, S, C, I, and A. \"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string.   \"\n",
    "    \"Valid options for each metric are as follows: \\n\"\n",
    "    \"- **Attack Vector (AV)**: Network (N), Adjacent (A), Local (L), Physical (P)\\n\"\n",
    "    \"- **Attack Complexity (AC)**: Low (L), High (H)\\n\"\n",
    "    \"- **Privileges Required (PR)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **User Interaction (UI)**: None (N), Required (R)\\n\"\n",
    "    \"- **Scope (S)**: Unchanged (U), Changed (C)\\n\"\n",
    "    \"- **Confidentiality (C)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Integrity (I)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Availability (A)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string. \"\n",
    "    \"Ensure the final line of your response contains ONLY the CVSS v3 Vector String (no other text) \"\n",
    "    \"in the following format:  \\n\"\n",
    "    \"Example format: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\\n\\n\"\n",
    "    \"CVE Description: In the Linux kernel through 6.7.1, there is a use-after-free \"\n",
    "    \"in cec_queue_msg_fh, related to drivers/media/cec/core/cec-adap.c and \"\n",
    "    \"drivers/media/cec/core/cec-api.c.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b473c-9b93-46f9-9ac5-6fb8e3fe314c",
   "metadata": {},
   "source": [
    "##### Are all the APIS working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5ac66-c0a8-4a32-81b0-4c45d68092ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(get_single_prediction(question, 'api-llama3.1-8b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c673-8d44-43b7-a04a-73f1ec36e2a9",
   "metadata": {},
   "source": [
    "# Run Evaluation for a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580bd3-20e4-4952-af00-e49810c7703c",
   "metadata": {},
   "source": [
    "### All formatting comes here\n",
    "While these captures most output format of the LLMs, someimtes have to manually collect some responses from the generated response file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c5b73-af68-443f-8b80-f1cfec5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(text):\n",
    "    # Define the regex pattern for CVSS v3.1 vector string\n",
    "    cvss_pattern = r'AV:[A-Za-z]+/AC:[A-Za-z]+/PR:[A-Za-z]+/UI:[A-Za-z]+/S:[A-Za-z]+/C:[A-Za-z]+/I:[A-Za-z]+/A:[A-Za-z]+'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cvss_pattern, text)\n",
    "\n",
    "    # Return the last match if any match is found, otherwise return the original text\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        return text, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6068fb-49a8-4d80-b7d4-0d5f58750a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path, model_name):\n",
    "    # Keep track of time and total #chars generated\n",
    "    start_time = time.time()\n",
    "    count_chars = 0\n",
    "    instructions_failed = 0\n",
    "    \n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    # Only keep results\n",
    "    all_results = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        try:\n",
    "            output = get_single_prediction(prompt, model_name)\n",
    "            count_chars += len(output)\n",
    "            answer, success = format(output)\n",
    "            if not success:\n",
    "                instructions_failed += 1\n",
    "            all_results.append(answer)\n",
    "            print(index+1, answer)\n",
    "        except Exception as e:\n",
    "            answer = 'Error'\n",
    "            all_results.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "            print(index+1, answer)\n",
    "\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Create responses directory if it doesn't exist\n",
    "    output_dir = os.path.join('responses', 'individual-results')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Only save the results to the responses folder\n",
    "    out_result = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_' + model_name + '_result.txt')\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3123f72-0838-4fc1-8ebe-912219909344",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation('../datasets/cti-vsp-only-2024-and-2025-SMALL.tsv', 'api-llama3.1-8b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
