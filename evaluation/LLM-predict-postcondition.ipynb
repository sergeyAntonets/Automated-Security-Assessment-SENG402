{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4af50b0",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2783da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from runningLLAMA import llama_local_generate\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda94d8f",
   "metadata": {},
   "source": [
    "## Prepare the prompts ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeb8be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are given a vulnerability description and a CVSS vector. Your task is to determine the post-condition privilege level â€” the level of access the attacker gains *after* successful exploitation.\n",
    "\n",
    "Classify the post-condition privilege as one of the following:\n",
    "\n",
    "- None: Attacker gains no additional access or capability.\n",
    "- User: Attacker gains user-level access (e.g., running code as a normal user, accessing user files).\n",
    "- Root: Attacker gains full system or administrative access (e.g., root privileges, complete control over the system or application).\n",
    "\n",
    "Instructions:\n",
    "- Use both the CVE description and CVSS vector to make your decision.\n",
    "- Briefly justify your classification.\n",
    "- Your final output must be in two parts:\n",
    "  1. A short explanation (1-2 sentences)\n",
    "  2. A single line containing only one of: `None`, `User`, or `Root`\n",
    "\n",
    "Example format:\n",
    "\n",
    "Explanation of reasoning...\n",
    "\n",
    "<Privilege>\n",
    "\n",
    "Now process the following:\n",
    "\"\"\"\n",
    "# Append the following\n",
    "# CVE Description: <Insert CVE description here>  \n",
    "# CVSS Vector: <Insert CVSS vector here>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "407d6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.1\n",
    "top_p = 1\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73636c19",
   "metadata": {},
   "source": [
    "## Prompt the LLM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29f860d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_post_condition(text):\n",
    "    \"\"\"\n",
    "    Extract post-condition privilege classification from LLM response text.\n",
    "    Returns the last valid classification found and whether extraction was successful.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for matching privilege classification\n",
    "    # It must be exactly 'None', 'User', or 'Root' on a line by itself (case-sensitive)\n",
    "    privilege_pattern = r'^(None|User|Root)\\s*$'\n",
    "\n",
    "    # Split the text into lines (from bottom up) and search for a matching line\n",
    "    lines = text.strip().splitlines()\n",
    "\n",
    "    for line in reversed(lines):\n",
    "        if re.match(privilege_pattern, line.strip()):\n",
    "            return line.strip(), True\n",
    "\n",
    "    # If no valid classification found\n",
    "    return text, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10708ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path):\n",
    "    \"\"\"\n",
    "    Run CVSS prediction evaluation on a dataset using specified model.\n",
    "    Processes each CVE description, extracts CVSS vectors, and saves results.\n",
    "    \"\"\"\n",
    "    # Track performance metrics for the evaluation run\n",
    "    start_time = time.time()\n",
    "    count_chars = 0  # Total characters generated by the model\n",
    "    instructions_failed = 0  # Count of responses that didn't follow CVSS format\n",
    "    \n",
    "    # Load the dataset (TSV format with CVE descriptions and prompts)\n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    all_results = []\n",
    "    all_full_responses = []  # Store full LLM responses for analysis\n",
    "    \n",
    "    # Process each row in the dataset\n",
    "    for index, row in data.iterrows():\n",
    "        llm_prompt = prompt + \" CVE Description: \" + row['Description'] + \" CVSS Vector: \" + row['CVSS']\n",
    "        try:\n",
    "            # Get prediction from the specified model\n",
    "            output = llama_local_generate(sys_prompt, llm_prompt, max_tokens,temperature, top_p, seed)\n",
    "            count_chars += len(output)\n",
    "            \n",
    "            # Store the full response\n",
    "            all_full_responses.append(f\"=== CVE {index+1} ===\\n{output}\\n\")\n",
    "            \n",
    "            # Try to extract CVSS vector from the response\n",
    "            answer, success = format_post_condition(output)\n",
    "            if not success:\n",
    "                instructions_failed += 1  # Model didn't follow CVSS format instructions\n",
    "            \n",
    "            all_results.append(answer)\n",
    "            print(index+1, answer)\n",
    "        except Exception as e:\n",
    "            # Handle any API errors or model failures\n",
    "            answer = 'Error'\n",
    "            error_msg = f\"=== CVE {index+1} ===\\nERROR: {str(e)}\\n\"\n",
    "            all_full_responses.append(error_msg)\n",
    "            all_results.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "\n",
    "    # Calculate and display performance metricsprompt\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Ensure output directory structure exists\n",
    "    output_dir = os.path.join('responses', 'individual-results')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save extracted results to file with standardized naming convention\n",
    "    out_result = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_postcondition.txt')\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    # Save full LLM responses for analysis and debugging\n",
    "    out_full_responses = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_full_postcondition_responses.txt')\n",
    "    with open(out_full_responses, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_full_responses))\n",
    "    \n",
    "    print(f'Results saved to: {out_result}')\n",
    "    print(f'Full responses saved to: {out_full_responses}')\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff188b",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad34da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Root\n",
      "2 Root\n",
      "2 Root\n",
      "3 User\n",
      "3 User\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data_set_file_path = \u001b[33m\"\u001b[39m\u001b[33m../datasets/postcondition-prediction.tsv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrun_evaluation\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     19\u001b[39m llm_prompt = prompt + \u001b[33m\"\u001b[39m\u001b[33m CVE Description: \u001b[39m\u001b[33m\"\u001b[39m + row[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m CVSS Vector: \u001b[39m\u001b[33m\"\u001b[39m + row[\u001b[33m'\u001b[39m\u001b[33mCVSS\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Get prediction from the specified model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     output = \u001b[43mllama_local_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     count_chars += \u001b[38;5;28mlen\u001b[39m(output)\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Store the full response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/evaluation/runningLLAMA.py:56\u001b[39m, in \u001b[36mllama_local_generate\u001b[39m\u001b[34m(sys_prompt, question, max_tokens, temperature, top_p, seed)\u001b[39m\n\u001b[32m     49\u001b[39m inputs_templated = tokenizer.apply_chat_template(\n\u001b[32m     50\u001b[39m     messages,\n\u001b[32m     51\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     53\u001b[39m ).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_templated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the templated input_ids\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Explicitly set pad_token_id\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Decode only the newly generated tokens\u001b[39;00m\n\u001b[32m     67\u001b[39m generated_tokens = outputs[\u001b[32m0\u001b[39m][inputs_templated.shape[-\u001b[32m1\u001b[39m]:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3422\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3420\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3422\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3423\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   3424\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   3426\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_set_file_path = \"../datasets/postcondition-prediction.tsv\"\n",
    "run_evaluation(data_set_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
