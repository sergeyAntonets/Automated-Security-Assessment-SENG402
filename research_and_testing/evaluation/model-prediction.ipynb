{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b6af60-f174-4dc7-a6b8-2ad51d8af784",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, LLAMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76192d8",
   "metadata": {},
   "source": [
    "### Install All Required Packages\n",
    "Run this cell first to install all necessary packages if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install openai replicate python-dotenv pandas numpy matplotlib google.genai transformers torch bitsandbytes llamaapi accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b88102b-a07f-4763-9fcb-a2bd248d00e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1967\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:40\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_rope_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ROPE_INIT_FUNCTIONS, dynamic_rope_update\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unpack\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:63\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflash_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attention_forward\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flex_attention_forward\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdpa_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/integrations/flex_attention.py:39\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_flex_attn_available():\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockMask, flex_attention\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     41\u001b[39m         create_block_mask \u001b[38;5;28;01mas\u001b[39;00m create_block_causal_mask_flex,\n\u001b[32m     42\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:15\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_trace_wrapped_higher_order_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformGetItemToIndex\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flex_attention \u001b[38;5;28;01mas\u001b[39;00m flex_attention_hop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_dynamo/__init__.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:52\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:52\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_dynamo/exc.py:41\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:68\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_functorch/config.py:40\u001b[39m\n\u001b[32m     38\u001b[39m cse = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[32m     43\u001b[39m enable_autograd_cache: \u001b[38;5;28mbool\u001b[39m = Config(\n\u001b[32m     44\u001b[39m     justknob=\u001b[33m\"\u001b[39m\u001b[33mpytorch/remote_cache:enable_local_autograd_cache\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     env_name_force=\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_AUTOGRAD_CACHE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     default=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_inductor/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, IO, Optional, TYPE_CHECKING, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/_inductor/config.py:467\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Disabled by default on ROCm, opt-in if model utilises NHWC convolutions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m layout_opt_default = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m.hip \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m layout_optimization = (\n\u001b[32m    469\u001b[39m     os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_LAYOUT_OPTIMIZATION\u001b[39m\u001b[33m\"\u001b[39m, layout_opt_default) == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'version'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllamaapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaAPI\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrunningLLAMA\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llama_local_generate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/research_and_testing/evaluation/runningLLAMA.py:29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model from Hugging Face\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Model is loaded with quantization and device mapping\u001b[39;00m\n\u001b[32m     28\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Set pad token for tokenizer compatibility\u001b[39;00m\n\u001b[32m     36\u001b[39m tokenizer.pad_token = tokenizer.eos_token \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:568\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    565\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:388\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    390\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:770\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    769\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    773\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:784\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    783\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:700\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1955\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1953\u001b[39m     value = Placeholder\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1969\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1970\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1971\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1972\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "from runningLLAMA import llama_local_generate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd791ee-3c67-4eb6-8678-827dfbf3bbb7",
   "metadata": {},
   "source": [
    "## Setup all APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d6a19-2b3a-4918-8c69-98313903ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10403f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama api\n",
    "llama_client = OpenAI(\n",
    "api_key = os.getenv(\"LLAMA_API_KEY\"),\n",
    "base_url = \"https://api.llmapi.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787e703-fd56-4d6f-82d1-e00ef4137503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPEN_AI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0eb4ea-6fea-407a-8de1-7f2ed22b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "gemini_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f299-880a-45d6-88a4-7b33448a7fb3",
   "metadata": {},
   "source": [
    "## LLM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f2a9-a4d2-4d57-b043-9873d213dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for deterministic and consistent model outputs across different LLMs\n",
    "# Low temperature and top_p reduce randomness, seed ensures reproducibility\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e074a-85de-48c7-982e-339dbce560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb01b07-3377-4845-a5a9-74719a86ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map internal model identifiers to actual API model names\n",
    "model_mapping = {\n",
    "    'api-llama3.1': 'llama3.1-8b',\n",
    "    'api-llama3.3': 'llama3.3-70b',\n",
    "    'gemini': 'gemini-2.0-flash', \n",
    "    'gpt-4o-mini': 'gpt-4o-mini',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4890ce6-7c73-4512-9316-64d49cc20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(question, model_name):\n",
    "    \"\"\"\n",
    "    Get a single prediction from various LLM providers based on model name.\n",
    "    Handles OpenAI GPT, Google Gemini, local LLAMA, and API-based LLAMA models.\n",
    "    \"\"\"\n",
    "    if model_name.startswith('gpt'):\n",
    "        # ChatGPT API call with parameters\n",
    "        model = model_mapping[model_name]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model_name.startswith('gemini'):\n",
    "        # Gemini API with safety settings and retry logic for rate limiting\n",
    "        model = model_mapping[model_name]\n",
    "\n",
    "        # Create message content combining system prompt and user question\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=sys_prompt + \" \" + question)])\n",
    "        ]        \n",
    "        \n",
    "        # Configure safety settings to allow more content through\n",
    "        safety_settings = [\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "        ]\n",
    "       \n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_output_tokens=max_tokens,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "       \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        )\n",
    "\n",
    "        # Retry logic to handle rate limiting (429 errors)\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = gemini_client.models.generate_content(\n",
    "                    model=model,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "                output = response.candidates[0].content.parts[0].text\n",
    "                time.sleep(1)  # Regular delay between requests\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"429 RESOURCE_EXHAUSTED\" in error_str:\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit hit, retrying in 2 seconds... (attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds as suggested by API\n",
    "                    if retry_count == max_retries:\n",
    "                        output = f\"Error: Rate limit exceeded after {max_retries} attempts.\"\n",
    "                else:\n",
    "                    output = f\"Error: {str(e)}\"\n",
    "                    break\n",
    "                \n",
    "    elif model_name.startswith('llama-local'):\n",
    "        # Local LLAMA model - temperature must be > 0 to avoid errors\n",
    "        output = llama_local_generate(sys_prompt, question, max_tokens=max_tokens, temperature=0.01, top_p=top_p, seed=seed)\n",
    "\n",
    "    elif model_name.startswith('api-llama'):\n",
    "         # API-based LLAMA models through LLMapi service\n",
    "         model = model_mapping[model_name]\n",
    "         response = llama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=0.01,  # Slightly above 0 for LLAMA models\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "         output = response.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported or not found in model_mapping\")\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb481fe-ff69-42af-8306-4eb4f5d48973",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd8376-21b9-4c91-b86d-876f1da04260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt to see if the API calls are working correctly\n",
    "question = (\n",
    "    \"Analyze the following CVE description and calculate the CVSS v3.1 Base Score. \"\n",
    "    \"Determine the values for each base metric: AV, AC, PR, UI, S, C, I, and A. \"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string.   \"\n",
    "    \"Valid options for each metric are as follows: \\n\"\n",
    "    \"- **Attack Vector (AV)**: Network (N), Adjacent (A), Local (L), Physical (P)\\n\"\n",
    "    \"- **Attack Complexity (AC)**: Low (L), High (H)\\n\"\n",
    "    \"- **Privileges Required (PR)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **User Interaction (UI)**: None (N), Required (R)\\n\"\n",
    "    \"- **Scope (S)**: Unchanged (U), Changed (C)\\n\"\n",
    "    \"- **Confidentiality (C)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Integrity (I)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Availability (A)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string. \"\n",
    "    \"Ensure the final line of your response contains ONLY the CVSS v3 Vector String (no other text) \"\n",
    "    \"in the following format:  \\n\"\n",
    "    \"Example format: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\\n\\n\"\n",
    "    \"CVE Description: In the Linux kernel through 6.7.1, there is a use-after-free \"\n",
    "    \"in cec_queue_msg_fh, related to drivers/media/cec/core/cec-adap.c and \"\n",
    "    \"drivers/media/cec/core/cec-api.c.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c5ac66-c0a8-4a32-81b0-4c45d68092ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_local_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_single_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama-local\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mget_single_prediction\u001b[39m\u001b[34m(question, model_name)\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name.startswith(\u001b[33m'\u001b[39m\u001b[33mllama-local\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Local LLAMA model - temperature must be > 0 to avoid errors\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     output = \u001b[43mllama_local_generate\u001b[49m(sys_prompt, question, max_tokens=max_tokens, temperature=\u001b[32m0.01\u001b[39m, top_p=top_p, seed=seed)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name.startswith(\u001b[33m'\u001b[39m\u001b[33mapi-llama\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     81\u001b[39m      \u001b[38;5;66;03m# API-based LLAMA models through LLMapi service\u001b[39;00m\n\u001b[32m     82\u001b[39m      model = model_mapping[model_name]\n",
      "\u001b[31mNameError\u001b[39m: name 'llama_local_generate' is not defined"
     ]
    }
   ],
   "source": [
    "print(get_single_prediction(question, 'llama-local'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c673-8d44-43b7-a04a-73f1ec36e2a9",
   "metadata": {},
   "source": [
    "# Run Evaluation on Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580bd3-20e4-4952-af00-e49810c7703c",
   "metadata": {},
   "source": [
    "While this captures most output format of the LLMs, sometimes have to manually collect some responses from the generated response file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "176c5b73-af68-443f-8b80-f1cfec5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(text):\n",
    "    \"\"\"\n",
    "    Extract CVSS v3.1 vector string from LLM response text.\n",
    "    Returns the last valid CVSS vector found and whether extraction was successful.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for CVSS v3.1 vector string format\n",
    "    # Matches: AV:X/AC:X/PR:X/UI:X/S:X/C:X/I:X/A:X where X can be letters\n",
    "    cvss_pattern = r'AV:[A-Za-z]+/AC:[A-Za-z]+/PR:[A-Za-z]+/UI:[A-Za-z]+/S:[A-Za-z]+/C:[A-Za-z]+/I:[A-Za-z]+/A:[A-Za-z]+'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cvss_pattern, text)\n",
    "\n",
    "    # Return the last match (most likely to be the final answer) if any match is found\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        # Return original text if no valid CVSS vector found (indicates parsing failure)\n",
    "        return text, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc6068fb-49a8-4d80-b7d4-0d5f58750a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path, model_name):\n",
    "    \"\"\"\n",
    "    Run CVSS prediction evaluation on a dataset using specified model.\n",
    "    Processes each CVE description, extracts CVSS vectors, and saves results.\n",
    "    \"\"\"\n",
    "    # Track performance metrics for the evaluation run\n",
    "    start_time = time.time()\n",
    "    count_chars = 0  # Total characters generated by the model\n",
    "    instructions_failed = 0  # Count of responses that didn't follow CVSS format\n",
    "    \n",
    "    # Load the dataset (TSV format with CVE descriptions and prompts)\n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each row in the dataset\n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        try:\n",
    "            # Get prediction from the specified model\n",
    "            output = get_single_prediction(prompt, model_name)\n",
    "            count_chars += len(output)\n",
    "            \n",
    "            # Try to extract CVSS vector from the response\n",
    "            answer, success = format(output)\n",
    "            if not success:\n",
    "                instructions_failed += 1  # Model didn't follow CVSS format instructions\n",
    "            \n",
    "            all_results.append(answer)\n",
    "            print(index+1, answer)\n",
    "        except Exception as e:\n",
    "            # Handle any API errors or model failures\n",
    "            answer = 'Error'\n",
    "            all_results.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "            print(index+1, answer)\n",
    "\n",
    "    # Calculate and display performance metrics\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Ensure output directory structure exists\n",
    "    output_dir = os.path.join('responses', 'individual-results')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save results to file with standardized naming convention\n",
    "    # Format: SENG402_<dataset-name>_<model-name>_result.txt\n",
    "    out_result = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_' + model_name + '_result.txt')\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3123f72-0838-4fc1-8ebe-912219909344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n",
      "2 AV:N/AC:L/PR:L/UI:R/S:C/C:L/I:L/A:N\n",
      "3 AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:H/A:N\n",
      "4 AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N\n",
      "5 AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H\n",
      "6 AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H\n",
      "Time taken: 40.646018505096436\n",
      "#Characters generated: 10827\n",
      "#Instructions failed: 0\n",
      "------- Done --------\n"
     ]
    }
   ],
   "source": [
    "run_evaluation('../datasets/2024-and-2025-SMALL.tsv', 'gemini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
