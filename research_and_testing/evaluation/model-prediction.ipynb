{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b6af60-f174-4dc7-a6b8-2ad51d8af784",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, LLAMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76192d8",
   "metadata": {},
   "source": [
    "### Install All Required Packages\n",
    "Run this cell first to install all necessary packages if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install openai replicate python-dotenv pandas numpy matplotlib google.genai transformers torch bitsandbytes llamaapi accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9372cb3",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, LLAMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b88102b-a07f-4763-9fcb-a2bd248d00e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b357116368664694a645d7ce148ab8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 11.62 GiB of which 1002.44 MiB is free. Process 1290068 has 5.64 GiB memory in use. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Of the allocated memory 4.60 GiB is allocated by PyTorch, and 86.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllamaapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaAPI\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrunningLLAMA\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llama_local_generate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/evaluation/runningLLAMA.py:29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model from Hugging Face\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Model is loaded with quantization and device mapping\u001b[39;00m\n\u001b[32m     28\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Set pad token for tokenizer compatibility\u001b[39;00m\n\u001b[32m     36\u001b[39m tokenizer.pad_token = tokenizer.eos_token \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Y4/SENG402/cti-bench-SENG402/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:765\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    763\u001b[39m     param = file_pointer.get_slice(serialized_param_name)\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     param = \u001b[43mempty_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n\u001b[32m    767\u001b[39m to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[32m    768\u001b[39m     model,\n\u001b[32m    769\u001b[39m     param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    772\u001b[39m     hf_quantizer,\n\u001b[32m    773\u001b[39m )\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# In this case, the param is already on the correct device!\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 11.62 GiB of which 1002.44 MiB is free. Process 1290068 has 5.64 GiB memory in use. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Of the allocated memory 4.60 GiB is allocated by PyTorch, and 86.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "from runningLLAMA import llama_local_generate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd791ee-3c67-4eb6-8678-827dfbf3bbb7",
   "metadata": {},
   "source": [
    "## Setup all APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d6a19-2b3a-4918-8c69-98313903ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10403f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama api\n",
    "llama_client = OpenAI(\n",
    "api_key = os.getenv(\"LLAMA_API_KEY\"),\n",
    "base_url = \"https://api.llmapi.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787e703-fd56-4d6f-82d1-e00ef4137503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPEN_AI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0eb4ea-6fea-407a-8de1-7f2ed22b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "gemini_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f299-880a-45d6-88a4-7b33448a7fb3",
   "metadata": {},
   "source": [
    "## LLM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f2a9-a4d2-4d57-b043-9873d213dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for deterministic and consistent model outputs across different LLMs\n",
    "# Low temperature and top_p reduce randomness, seed ensures reproducibility\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e074a-85de-48c7-982e-339dbce560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb01b07-3377-4845-a5a9-74719a86ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map internal model identifiers to actual API model names\n",
    "model_mapping = {\n",
    "    'api-llama3.1': 'llama3.1-8b',\n",
    "    'api-llama3.3': 'llama3.3-70b',\n",
    "    'gemini': 'gemini-2.0-flash', \n",
    "    'gpt-4o-mini': 'gpt-4o-mini',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4890ce6-7c73-4512-9316-64d49cc20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(question, model_name):\n",
    "    \"\"\"\n",
    "    Get a single prediction from various LLM providers based on model name.\n",
    "    Handles OpenAI GPT, Google Gemini, local LLAMA, and API-based LLAMA models.\n",
    "    \"\"\"\n",
    "    if model_name.startswith('gpt'):\n",
    "        # ChatGPT API call with parameters\n",
    "        model = model_mapping[model_name]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model_name.startswith('gemini'):\n",
    "        # Gemini API with safety settings and retry logic for rate limiting\n",
    "        model = model_mapping[model_name]\n",
    "\n",
    "        # Create message content combining system prompt and user question\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=sys_prompt + \" \" + question)])\n",
    "        ]        \n",
    "        \n",
    "        # Configure safety settings to allow more content through\n",
    "        safety_settings = [\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "            types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH),\n",
    "        ]\n",
    "       \n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_output_tokens=max_tokens,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "       \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        )\n",
    "\n",
    "        # Retry logic to handle rate limiting (429 errors)\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = gemini_client.models.generate_content(\n",
    "                    model=model,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "                output = response.candidates[0].content.parts[0].text\n",
    "                time.sleep(1)  # Regular delay between requests\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"429 RESOURCE_EXHAUSTED\" in error_str:\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit hit, retrying in 2 seconds... (attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds as suggested by API\n",
    "                    if retry_count == max_retries:\n",
    "                        output = f\"Error: Rate limit exceeded after {max_retries} attempts.\"\n",
    "                else:\n",
    "                    output = f\"Error: {str(e)}\"\n",
    "                    break\n",
    "                \n",
    "    elif model_name.startswith('llama-local'):\n",
    "        # Local LLAMA model - temperature must be > 0 to avoid errors\n",
    "        output = llama_local_generate(sys_prompt, question, max_tokens=max_tokens, temperature=0.01, top_p=top_p, seed=seed)\n",
    "\n",
    "    elif model_name.startswith('api-llama'):\n",
    "         # API-based LLAMA models through LLMapi service\n",
    "         model = model_mapping[model_name]\n",
    "         response = llama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=0.01,  # Slightly above 0 for LLAMA models\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "         output = response.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported or not found in model_mapping\")\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb481fe-ff69-42af-8306-4eb4f5d48973",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd8376-21b9-4c91-b86d-876f1da04260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt to see if the API calls are working correctly\n",
    "question = (\n",
    "    \"Analyze the following CVE description and calculate the CVSS v3.1 Base Score. \"\n",
    "    \"Determine the values for each base metric: AV, AC, PR, UI, S, C, I, and A. \"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string.   \"\n",
    "    \"Valid options for each metric are as follows: \\n\"\n",
    "    \"- **Attack Vector (AV)**: Network (N), Adjacent (A), Local (L), Physical (P)\\n\"\n",
    "    \"- **Attack Complexity (AC)**: Low (L), High (H)\\n\"\n",
    "    \"- **Privileges Required (PR)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **User Interaction (UI)**: None (N), Required (R)\\n\"\n",
    "    \"- **Scope (S)**: Unchanged (U), Changed (C)\\n\"\n",
    "    \"- **Confidentiality (C)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Integrity (I)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"- **Availability (A)**: None (N), Low (L), High (H)\\n\"\n",
    "    \"Summarize each metric's value and provide the final CVSS v3.1 vector string. \"\n",
    "    \"Ensure the final line of your response contains ONLY the CVSS v3 Vector String (no other text) \"\n",
    "    \"in the following format:  \\n\"\n",
    "    \"Example format: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\\n\\n\"\n",
    "    \"CVE Description: In the Linux kernel through 6.7.1, there is a use-after-free \"\n",
    "    \"in cec_queue_msg_fh, related to drivers/media/cec/core/cec-adap.c and \"\n",
    "    \"drivers/media/cec/core/cec-api.c.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c5ac66-c0a8-4a32-81b0-4c45d68092ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_local_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_single_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama-local\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mget_single_prediction\u001b[39m\u001b[34m(question, model_name)\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name.startswith(\u001b[33m'\u001b[39m\u001b[33mllama-local\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Local LLAMA model - temperature must be > 0 to avoid errors\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     output = \u001b[43mllama_local_generate\u001b[49m(sys_prompt, question, max_tokens=max_tokens, temperature=\u001b[32m0.01\u001b[39m, top_p=top_p, seed=seed)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name.startswith(\u001b[33m'\u001b[39m\u001b[33mapi-llama\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     81\u001b[39m      \u001b[38;5;66;03m# API-based LLAMA models through LLMapi service\u001b[39;00m\n\u001b[32m     82\u001b[39m      model = model_mapping[model_name]\n",
      "\u001b[31mNameError\u001b[39m: name 'llama_local_generate' is not defined"
     ]
    }
   ],
   "source": [
    "print(get_single_prediction(question, 'llama-local'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c673-8d44-43b7-a04a-73f1ec36e2a9",
   "metadata": {},
   "source": [
    "# Run Evaluation on Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580bd3-20e4-4952-af00-e49810c7703c",
   "metadata": {},
   "source": [
    "While this captures most output format of the LLMs, sometimes have to manually collect some responses from the generated response file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "176c5b73-af68-443f-8b80-f1cfec5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(text):\n",
    "    \"\"\"\n",
    "    Extract CVSS v3.1 vector string from LLM response text.\n",
    "    Returns the last valid CVSS vector found and whether extraction was successful.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for CVSS v3.1 vector string format\n",
    "    # Matches: AV:X/AC:X/PR:X/UI:X/S:X/C:X/I:X/A:X where X can be letters\n",
    "    cvss_pattern = r'AV:[A-Za-z]+/AC:[A-Za-z]+/PR:[A-Za-z]+/UI:[A-Za-z]+/S:[A-Za-z]+/C:[A-Za-z]+/I:[A-Za-z]+/A:[A-Za-z]+'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cvss_pattern, text)\n",
    "\n",
    "    # Return the last match (most likely to be the final answer) if any match is found\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        # Return original text if no valid CVSS vector found (indicates parsing failure)\n",
    "        return text, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc6068fb-49a8-4d80-b7d4-0d5f58750a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path, model_name):\n",
    "    \"\"\"\n",
    "    Run CVSS prediction evaluation on a dataset using specified model.\n",
    "    Processes each CVE description, extracts CVSS vectors, and saves results.\n",
    "    \"\"\"\n",
    "    # Track performance metrics for the evaluation run\n",
    "    start_time = time.time()\n",
    "    count_chars = 0  # Total characters generated by the model\n",
    "    instructions_failed = 0  # Count of responses that didn't follow CVSS format\n",
    "    \n",
    "    # Load the dataset (TSV format with CVE descriptions and prompts)\n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each row in the dataset\n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        try:\n",
    "            # Get prediction from the specified model\n",
    "            output = get_single_prediction(prompt, model_name)\n",
    "            count_chars += len(output)\n",
    "            \n",
    "            # Try to extract CVSS vector from the response\n",
    "            answer, success = format(output)\n",
    "            if not success:\n",
    "                instructions_failed += 1  # Model didn't follow CVSS format instructions\n",
    "            \n",
    "            all_results.append(answer)\n",
    "            print(index+1, answer)\n",
    "        except Exception as e:\n",
    "            # Handle any API errors or model failures\n",
    "            answer = 'Error'\n",
    "            all_results.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "            print(index+1, answer)\n",
    "\n",
    "    # Calculate and display performance metrics\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Ensure output directory structure exists\n",
    "    output_dir = os.path.join('responses', 'individual-results')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save results to file with standardized naming convention\n",
    "    # Format: SENG402_<dataset-name>_<model-name>_result.txt\n",
    "    out_result = os.path.join(output_dir, 'SENG402_' + os.path.basename(file_path).split('.')[0] + '_' + model_name + '_result.txt')\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3123f72-0838-4fc1-8ebe-912219909344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\n",
      "2 AV:N/AC:L/PR:L/UI:R/S:C/C:L/I:L/A:N\n",
      "3 AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:H/A:N\n",
      "4 AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N\n",
      "5 AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H\n",
      "6 AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H\n",
      "Time taken: 40.646018505096436\n",
      "#Characters generated: 10827\n",
      "#Instructions failed: 0\n",
      "------- Done --------\n"
     ]
    }
   ],
   "source": [
    "run_evaluation('../datasets/2024-and-2025-SMALL.tsv', 'gemini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
